{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEPKkkTgrDd8",
        "outputId": "cba43a00-dd0b-403d-dbb2-893157bd7eb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cxw-SeBQs4Xy",
        "outputId": "63dcdc22-0852-43e1-9236-6a9dfc26771d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.108-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Collecting opencv-python>=4.6.0 (from ultralytics)\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cpu)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Collecting py-cpuinfo (from ultralytics)\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.108-py3-none-any.whl (974 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m974.8/974.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: py-cpuinfo, opencv-python, ultralytics-thop, ultralytics\n",
            "Successfully installed opencv-python-4.11.0.86 py-cpuinfo-9.0.0 ultralytics-8.3.108 ultralytics-thop-2.0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load YOLOv8 model and export to TorchScript\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Path to your original YOLOv8 model in Drive\n",
        "model_path = \"/content/drive/MyDrive/saved_model/best.pt\"\n",
        "export_path = \"/content/yolov8n.torchscript.pt\"\n",
        "\n",
        "# Load the model\n",
        "model = YOLO(model_path)\n",
        "\n",
        "# Export to TorchScript format\n",
        "model.export(format=\"torchscript\", device='cpu', simplify=True, path=export_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "-epj4ePywyUe",
        "outputId": "5b8ad263-5653-41a3-e1ca-08a7178c5d91"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "'\u001b[31m\u001b[1mpath\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['batch=16'].\n\n    Arguments received: ['yolo', '-f', '/root/.local/share/jupyter/runtime/kernel-1c4b5a08-237e-48fd-8b17-5f1e34bf80c3.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of frozenset({'classify', 'obb', 'detect', 'pose', 'segment'})\n                MODE (required) is one of frozenset({'benchmark', 'track', 'val', 'export', 'predict', 'train'})\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n\n    5. Ultralytics solutions usage\n        yolo solutions count or in ['crop', 'blur', 'workout', 'heatmap', 'isegment', 'visioneye', 'speed', 'queue', 'analytics', 'inference', 'trackzone'] source=\"path/to/video.mp4\"\n\n    6. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n        yolo solutions help\n\n    Docs: https://docs.ultralytics.com\n    Solutions: https://docs.ultralytics.com/solutions/\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n     (<string>)",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \u001b[1;32m\"<ipython-input-8-86a6914542a0>\"\u001b[0m, line \u001b[1;32m12\u001b[0m, in \u001b[1;35m<cell line: 0>\u001b[0m\n    model.export(format=\"torchscript\", device='cpu', simplify=True, path=export_path)\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\"\u001b[0m, line \u001b[1;32m728\u001b[0m, in \u001b[1;35mexport\u001b[0m\n    return Exporter(overrides=args, _callbacks=self.callbacks)(model=self.model)\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/exporter.py\"\u001b[0m, line \u001b[1;32m240\u001b[0m, in \u001b[1;35m__init__\u001b[0m\n    self.args = get_cfg(cfg, overrides)\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.11/dist-packages/ultralytics/cfg/__init__.py\"\u001b[0m, line \u001b[1;32m309\u001b[0m, in \u001b[1;35mget_cfg\u001b[0m\n    check_dict_alignment(cfg, overrides)\n",
            "\u001b[0;36m  File \u001b[0;32m\"/usr/local/lib/python3.11/dist-packages/ultralytics/cfg/__init__.py\"\u001b[0;36m, line \u001b[0;32m499\u001b[0;36m, in \u001b[0;35mcheck_dict_alignment\u001b[0;36m\u001b[0m\n\u001b[0;31m    raise SyntaxError(string + CLI_HELP_MSG) from e\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '\u001b[31m\u001b[1mpath\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['batch=16'].\n\n    Arguments received: ['yolo', '-f', '/root/.local/share/jupyter/runtime/kernel-1c4b5a08-237e-48fd-8b17-5f1e34bf80c3.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of frozenset({'classify', 'obb', 'detect', 'pose', 'segment'})\n                MODE (required) is one of frozenset({'benchmark', 'track', 'val', 'export', 'predict', 'train'})\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n\n    5. Ultralytics solutions usage\n        yolo solutions count or in ['crop', 'blur', 'workout', 'heatmap', 'isegment', 'visioneye', 'speed', 'queue', 'analytics', 'inference', 'trackzone'] source=\"path/to/video.mp4\"\n\n    6. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n        yolo solutions help\n\n    Docs: https://docs.ultralytics.com\n    Solutions: https://docs.ultralytics.com/solutions/\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Loading YOLOv8 model from Google Drive\n",
        "model_path = \"/content/drive/MyDrive/saved_model/best.pt\"\n",
        "model = YOLO(model_path)\n",
        "\n",
        "\n",
        "# Converting to TorchScript ,no quantization yet\n",
        "model.export(format=\"torchscript\", device='cpu', simplify=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "g0EFj3yptniB",
        "outputId": "4d53a66a-8c27-4e12-c253-e9cf5349f0e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Ultralytics 8.3.108 üöÄ Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.00GHz)\n",
            "YOLOv8n-cls summary (fused): 30 layers, 1,454,095 parameters, 0 gradients, 3.3 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/drive/MyDrive/saved_model/best.pt' with input shape (1, 3, 224, 224) BCHW and output shape(s) (1, 15) (2.9 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.6.0+cu124...\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success ‚úÖ 1.6s, saved as '/content/drive/MyDrive/saved_model/best.torchscript' (5.7 MB)\n",
            "\n",
            "Export complete (3.3s)\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/saved_model\u001b[0m\n",
            "Predict:         yolo predict task=classify model=/content/drive/MyDrive/saved_model/best.torchscript imgsz=224  \n",
            "Validate:        yolo val task=classify model=/content/drive/MyDrive/saved_model/best.torchscript imgsz=224 data=yolo_classification_dataset  \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/saved_model/best.torchscript'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import torch\n",
        "import torch.quantization\n",
        "import shutil\n",
        "import os\n",
        "# Export to TorchScript (default location used)\n",
        "model.export(format=\"torchscript\", device=\"cpu\", simplify=True)\n",
        "\n",
        "# The exported path will be:\n",
        "exported_path = \"runs/torchscript/yolov8n/weights/best.torchscript.pt\"\n",
        "\n",
        "# Confirm file exists\n",
        "assert os.path.exists(exported_path), \"Export failed, file not found!\"\n",
        "\n",
        "# Load the TorchScript model\n",
        "scripted_model = torch.jit.load(exported_path)\n",
        "\n",
        "# Apply dynamic quantization\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    scripted_model,\n",
        "    {torch.nn.Linear},\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Save quantized model to Google Drive\n",
        "quantized_model_path = \"/content/drive/MyDrive/saved_model/yolov8n_quantized.torchscript.pt\"\n",
        "quantized_model.save(quantized_model_path)\n",
        "\n",
        "print(\"‚úÖ Quantized model saved to:\", quantized_model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "collapsed": true,
        "id": "M_iXDEg9WqKN",
        "outputId": "1a0c01ad-2d14-405a-def0-2916a5482f6a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-26c215acc3dc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Export to TorchScript (default location used)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"torchscript\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimplify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# The exported path will be:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import torch\n",
        "import torch.quantization\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Load model from Drive\n",
        "model = YOLO(\"/content/drive/MyDrive/saved_model/best.pt\")\n",
        "\n",
        "# Export to TorchScript (default saves to runs/torchscript)\n",
        "model.export(format=\"torchscript\", device=\"cpu\", simplify=True)\n",
        "\n",
        "# Find exported model automatically\n",
        "torchscript_files = glob.glob(\"runs/torchscript/**/*.pt\", recursive=True)\n",
        "assert torchscript_files, \" No TorchScript model found after export!\"\n",
        "exported_path = torchscript_files[0]\n",
        "print(\" Found TorchScript model at:\", exported_path)\n",
        "\n",
        "# Load the model\n",
        "scripted_model = torch.jit.load(exported_path)\n",
        "\n",
        "# Quantize\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    scripted_model,\n",
        "    {torch.nn.Linear},\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Save to Drive\n",
        "\n",
        "quantized_path = \"/content/drive/MyDrive/saved_model/yolov8n_quantized.torchscript.pt\"\n",
        "quantized_model.save(quantized_path)\n",
        "\n",
        "print(\" Quantized model saved to:\", quantized_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "u-qWjxxHXiTT",
        "outputId": "2d506f41-fc29-4a80-bccd-392e025b8323"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.108 üöÄ Python-3.11.12 torch-2.6.0+cpu CPU (Intel Xeon 2.00GHz)\n",
            "YOLOv8n-cls summary (fused): 30 layers, 1,454,095 parameters, 0 gradients, 3.3 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/drive/MyDrive/saved_model/best.pt' with input shape (1, 3, 224, 224) BCHW and output shape(s) (1, 15) (2.9 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.6.0+cpu...\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success ‚úÖ 1.4s, saved as '/content/drive/MyDrive/saved_model/best.torchscript' (5.7 MB)\n",
            "\n",
            "Export complete (2.9s)\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/saved_model\u001b[0m\n",
            "Predict:         yolo predict task=classify model=/content/drive/MyDrive/saved_model/best.torchscript imgsz=224  \n",
            "Validate:        yolo val task=classify model=/content/drive/MyDrive/saved_model/best.torchscript imgsz=224 data=yolo_classification_dataset  \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": " No TorchScript model found after export!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-380e48cc6729>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Find exported model automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtorchscript_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"runs/torchscript/**/*.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtorchscript_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" No TorchScript model found after export!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mexported_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchscript_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Found TorchScript model at:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexported_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m:  No TorchScript model found after export!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.quantization\n",
        "export_path = \"/content/yolov8n.torchscript.pt\"\n",
        "model.export(format=\"torchscript\")\n",
        "# Load the scripted model\n",
        "scripted_model = torch.jit.load(\"yolov8n.torchscript.pt\")\n",
        "\n",
        "# Dynamic quantization ‚Äì works only on nn.Linear layers\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    scripted_model,\n",
        "    {torch.nn.Linear},\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Save quantized model\n",
        "quantized_model_path = \"/content/drive/MyDrive/saved_model/yolov8n_quantized.torchscript.pt\"\n",
        "quantized_model.save(quantized_model_path)\n"
      ],
      "metadata": {
        "id": "W5cwqLJduCvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bWKxCbLUuiMJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}